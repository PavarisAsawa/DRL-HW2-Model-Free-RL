{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Part 2: Trainning & Playing to stabilize Cart-Pole Agent.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `Name` : **Pavaris Asawakijtananont**\n",
    "- `Number` : 65340500037"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Testing Configuration and Control Variable**\n",
    "\n",
    "In this part we start to training agent from small space and small action space to ensure that agent can visit all state and update all possible action to learn to stabilize cartpole.\n",
    "- at first we settup all algorithm with same configuration space consisting with\n",
    "    - number of action\n",
    "    - action range\n",
    "    - number of bins / number of state (for discretize to discrete space)\n",
    "    - number of episode\n",
    "    - start epsilon\n",
    "    - final epsilon\n",
    "    - epsilon decay rate\n",
    "    - learning rate\n",
    "    - discount factor\n",
    "    - boundary of state\n",
    "\n",
    "- configure for agent and learning algorithm\n",
    "```python\n",
    "    num_of_action = 5\n",
    "    action_range = [-12, 12]  # [min, max]\n",
    "    discretize_state_weight = [4, 8, 4, 4]  # [pose_cart:int, pose_pole:int, vel_cart:int, vel_pole:int]\n",
    "    learning_rate = 0.03\n",
    "    n_episodes = 5000\n",
    "    start_epsilon = 1.0\n",
    "    epsilon_decay = 0.00003 # reduce the exploration over time\n",
    "    final_epsilon = 0.05\n",
    "    discount = 1\n",
    "```\n",
    "- configure for discretize space\n",
    "```python\n",
    "    pose_cart_bound = 3\n",
    "    pose_pole_bound = float(np.deg2rad(24.0))\n",
    "    vel_cart_bound = 15\n",
    "    vel_pole_bound = 15\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Agent State Space**\n",
    "\n",
    "for ensure that agent can visit all possible state we set a small state space and calculating to a episode, the agent have\n",
    "\n",
    "$$\n",
    "N_S = S_{pose_cart}*S_{pose_pole}*S_{vel_cart}*S_{vel_cart} = 512\n",
    "\n",
    "N_Q = N_S * N_A = 2560\n",
    "$$\n",
    "\n",
    "all space of action value is less than episode that mean it is highly chance to visit most of all state and update action value of pair of state action many time. \n",
    "\n",
    "- for visit a state it depend on exploration and exploitation rate is decay epsilon rate that we use linear decay\n",
    "\n",
    "$$\n",
    "    1-decay \\cdot step = 0.05\n",
    "$$\n",
    "$$\n",
    "    step \\approx 3333\n",
    "$$\n",
    "\n",
    "the agent will start quite fully exploitation or update optimal action from learning after 3333 step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Base-line Experiment**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Reward Setting**\n",
    "- for first experiment we setting the same reward function, terminate conditions and all configuration in environment to test all algorithm.\n",
    "\n",
    "- you can see more in `stabilize_cartpole_env_cfg.py`\n",
    "\n",
    "```python\n",
    "class RewardsCfg:\n",
    "    \"\"\"Reward terms for the MDP.\"\"\"\n",
    "\n",
    "    # (1) Constant running reward\n",
    "    alive = RewTerm(func=mdp.is_alive, weight=1.0)\n",
    "    # (2) Failure penalty\n",
    "    terminating = RewTerm(func=mdp.is_terminated, weight=-2.0)\n",
    "    # (3) Primary task: keep pole upright\n",
    "    pole_pos = RewTerm(\n",
    "        func=mdp.joint_pos_target_l2,\n",
    "        weight=-1.0,\n",
    "        params={\"asset_cfg\": SceneEntityCfg(\"robot\", joint_names=[\"cart_to_pole\"]), \"target\": 0.0},\n",
    "    )\n",
    "```\n",
    "|Task | Reward| Description |\n",
    "|---|---|---|\n",
    "| Alive | 1 | agent aliving reward |\n",
    "| Terminate | -2 | penalty when terminate |\n",
    "| Pole Position | -1 | penalty when agent not in target (0)|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    # Clipping value\n",
    "    pose_cart_bound = 3\n",
    "    pose_pole_bound = float(np.deg2rad(24.0))\n",
    "    vel_cart_bound = 15\n",
    "    vel_pole_bound = 15\n",
    "    \n",
    "    num_of_action = 5\n",
    "    action_range = [-12, 12]  # [min, max]\n",
    "    discretize_state_weight = [4, 8, 4, 4]  # [pose_cart:int, pose_pole:int, vel_cart:int, vel_pole:int]\n",
    "    learning_rate = 0.03\n",
    "    n_episodes = 5000\n",
    "    start_epsilon = 0.05\n",
    "    epsilon_decay = 0.00003 # reduce the exploration over time\n",
    "    final_epsilon = 0.05\n",
    "    discount = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Q-Learning**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "    <video width=\"750\" controls>\n",
    "        <source src=\"video/simple_QL.mp4\" type=\"video/mp4\">\n",
    "    </video>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In `Q Learning` Cartpole can learn to stabilize but when time run the agent is moving to boundary of **[-3 ,3]** and agent is terminating, that mean agent is learn to stabilize but agent is still not fully learn to not out of bound, It's can from not enought velocity action to cartpole to swing the pole to opposite direction to change cart position."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Double Q-Learning**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "    <video width=\"750\" controls>\n",
    "        <source src=\"video/simple_DQL.mp4\" type=\"video/mp4\">\n",
    "    </video>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- agent learn with `Double Q Learning` also can learn to stabilzie too, but someway agent look like doesn't learn to not terminate by out of boundary, and agent look like got limit velocity and it can't swing the pole to another direction that make cart out of bound  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **SARSA**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "    <video width=\"750\" controls>\n",
    "        <source src=\"video/simple_SARSA.mp4\" type=\"video/mp4\">\n",
    "    </video>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `SARSA` is same as a Double Q Learning but `SARSA` is mostly learn to stabilize in to right direction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Monte Carlo**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "    <video width=\"750\" controls>\n",
    "        <source src=\"video/simple_MC0.mp4\" type=\"video/mp4\">\n",
    "    </video>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `Monte Carlo` is can't learn to stabilize with default configuration but it learn to **terminate** instead\n",
    "- to make agent learn with `Monte Carlo` we need to set some configuration is **`Discount Factor`** for more detail will discussion in **PART 3** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Changing Discount Factor**\n",
    "- I have reduce Discout Factor from 1 to 0.25 and 0.01 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"display: flex; justify-content: center; gap: 20px;\">\n",
    "    <video width=\"750\" controls>\n",
    "        <source src=\"video/simple_MC1.mp4\" type=\"video/mp4\">\n",
    "    </video>\n",
    "    <video width=\"750\" controls>\n",
    "        <source src=\"video/simple_MC2.mp4\" type=\"video/mp4\">\n",
    "    </video>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Left : Discout Factor = 0.25 , Right : Discount Factor = 0.01*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Discount Factor : 0.25** - agent can learn to stabilize but to perform stabilize still not good, the pole is swing over time to stabilize\n",
    "\n",
    "**Discount Factor : 0.01** - agent also learn to stabilize but cart can't change direction to not terminate, behavior of this agent is like a SARSA or Double Q learning but this agent can learn to stabilize with 2 direction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Summary**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Every algorithm can enable the agent to learn to stabilize the pole, but different algorithms perform differently under the same configuration. Each algorithm has its own pros and cons. For example, Monte Carlo is a high-variance method that can fail if we weight future rewards too heavily (or if the final reward at termination dominates). In contrast, methods that use bootstrapping to predict the next action and state from the current policy can introduce high bias. Despite these trade-offs, off-policy methods still manage to stabilize the pole.\n",
    "- **`Q-Learning`** is the best agent for the stabilization task; however, it also has a problem: the agent does not learn to change direction when the cart is about to go out of bounds, similar to other algorithms. To address this issue, increasing the range of actions to allow higher velocities might help swing the pole in the opposite direction. Currently, the cart is merely following the pole to stabilize it, which causes the cart to move out of bounds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Stabilize with larger space**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- in this experiment we will setting larger space in observation term and action space to improving stabilizing performance to observe performance of agent with difference number of training episode and improve stabilize performance\n",
    "\n",
    "**Independent Variable**\n",
    "- number of episode to train\n",
    "\n",
    "**Dependent Variable**\n",
    "- Performance in task(stabilize performance)\n",
    "- Q table (Part 3)\n",
    "\n",
    "**Control Variable**\n",
    "- Learning Algorithm : Q Learning\n",
    "- Environment Configuration\n",
    "- Reward Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "    # Hyper Parameter\n",
    "    num_of_action = 11\n",
    "    action_range = [-25, 25]  # [min, max]\n",
    "    discretize_state_weight = [7, 12, 5, 5]  # [pose_cart:int, pose_pole:int, vel_cart:int, vel_pole:int]\n",
    "    learning_rate = 0.03\n",
    "    n_episodes = 5000\n",
    "    start_epsilon = 1.0\n",
    "    epsilon_decay = 0.00003 # reduce the exploration over time\n",
    "    final_epsilon = 0.05\n",
    "    discount = 1\n",
    "\n",
    "    # Clipping value Parameter for discretize observation term\n",
    "    pose_cart_bound = 3\n",
    "    pose_pole_bound = float(np.deg2rad(24.0))\n",
    "    vel_cart_bound = 25\n",
    "    vel_pole_bound = 25\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Q Learning**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<div style=\"display: flex; justify-content: center; align-items: center;\">\n",
    "  <video width=\"750\" controls>\n",
    "    <source src=\"video/mod_QL_20000.mp4\" type=\"video/mp4\">\n",
    "  </video>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- When we expand the observation space (i.e., increase the state space available to the agent) and enlarge the action space along with its range, it affects the agent’s performance in stabilizing the pole. With the increased space, the agent learns to swing the cart back to the center to avoid termination. In contrast, with the previous configuration, the agent did not learn to return to the center and episodes frequently terminated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Double Q Learning**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Swing Up**"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
