{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Part 1: Setting up `Cart-Pole` Agent.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `Name` : **Pavaris Asawakijtananont**\n",
    "- `Number` : 65340500037"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1. RL Base class**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Constructor** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Core Function**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the Cart-Pole environment configuration file, we found that the observation space of the Cart-Pole consists of four components:\n",
    "\n",
    "1. Relative joint velocity of the cart.\n",
    "2. Relative joint velocity of the pole.\n",
    "3. Relative joint position of the cart.\n",
    "4. Relative joint position of the pole.\n",
    "\n",
    "Each component is continuous and has an $\\infty$ range, which results in high computational costs and an excessively large number of states. To discretize the observation state, we must first determine the complete range of values in the environment. Some of these parameters can be obtained from the Cart-Pole configuration file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "CARTPOLE_CFG = ArticulationCfg(\n",
    "    spawn=sim_utils.UsdFileCfg(\n",
    "        usd_path=f\"{ISAACLAB_NUCLEUS_DIR}/Robots/Classic/Cartpole/cartpole.usd\",\n",
    "        rigid_props=sim_utils.RigidBodyPropertiesCfg(\n",
    "            rigid_body_enabled=True,\n",
    "            max_linear_velocity=1000.0,\n",
    "            max_angular_velocity=1000.0,\n",
    "            max_depenetration_velocity=100.0,\n",
    "            enable_gyroscopic_forces=True,\n",
    "        ),\n",
    "        articulation_props=sim_utils.ArticulationRootPropertiesCfg(\n",
    "            enabled_self_collisions=False,\n",
    "            solver_position_iteration_count=4,\n",
    "            solver_velocity_iteration_count=0,\n",
    "            sleep_threshold=0.005,\n",
    "            stabilization_threshold=0.001,\n",
    "        ),\n",
    "    ),\n",
    "    init_state=ArticulationCfg.InitialStateCfg(\n",
    "        pos=(0.0, 0.0, 2.0), joint_pos={\"slider_to_cart\": 0.0, \"cart_to_pole\": 0.0}\n",
    "    ),\n",
    "    actuators={\n",
    "        \"cart_actuator\": ImplicitActuatorCfg(\n",
    "            joint_names_expr=[\"slider_to_cart\"],\n",
    "            effort_limit=400.0,\n",
    "            velocity_limit=100.0,\n",
    "            stiffness=0.0,\n",
    "            damping=10.0,\n",
    "        ),\n",
    "        \"pole_actuator\": ImplicitActuatorCfg(\n",
    "            joint_names_expr=[\"cart_to_pole\"], effort_limit=400.0, velocity_limit=100.0, stiffness=0.0, damping=0.0\n",
    "        ),\n",
    "    },\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. `pose_cart` : cart position we can set up on termination condition and we must have tolerance before terminate **[ -$\\pi$ , $\\pi$ ]** \n",
    "2. `pose_pol` : Isaac Sim have angle between **[ -$\\pi$ , $\\pi$ ]**\n",
    "3. `vel_cart` : velocity of cart is determine by `velocity_limit` of joint `slider_to_cart` is **[-100 , 100]**\n",
    "4. `vel_pol` : actuator set in **100 rad/s** \n",
    "\n",
    "all parameter must fine-tuning after training because sometime too much range will hard to discretize and too much state"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test , observation term start from 1 term , etc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **discretize_state(self , obs)**\n",
    "\n",
    "- Discretizes and scales the state based on observation weights.\n",
    "- to approximate state from continuos domain to discrete domain to estimate the state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "\n",
    "    def discretize_state(self, obs: dict):\n",
    "        \n",
    "        # define number of value\n",
    "        pose_cart_bin = self.discretize_state_weight[0]\n",
    "        pose_pole_bin = self.discretize_state_weight[1]\n",
    "        vel_cart_bin = self.discretize_state_weight[2]\n",
    "        vel_pole_bin = self.discretize_state_weight[3]\n",
    "\n",
    "        # Clipping value\n",
    "        pose_cart_bound = 4.5\n",
    "        pose_pole_bound = np.pi\n",
    "        vel_cart_bound = 10\n",
    "        vel_pole_bound = 10\n",
    "        \n",
    "        # get observation term from continuos space\n",
    "        pose_cart_raw, pose_pole_raw , vel_cart_raw , vel_pole_raw = obs['policy'][0, 0] , obs['policy'][0, 1] , obs['policy'][0, 2] , obs['policy'][0, 3]\n",
    "\n",
    "        pose_cart_clip = torch.clip(pose_cart_raw , -pose_cart_bound ,pose_cart_bound)\n",
    "        pose_pole_clip = torch.clip(pose_pole_raw , -pose_pole_bound ,pose_pole_bound)\n",
    "        vel_cart_clip = torch.clip(vel_cart_raw , -vel_cart_bound ,vel_cart_bound)\n",
    "        vel_pole_clip = torch.clip(vel_pole_raw , -vel_pole_bound ,vel_pole_bound)\n",
    "\n",
    "        device = pose_cart_clip.device\n",
    "\n",
    "        # linspace value\n",
    "        pose_cart_grid = torch.linspace(-pose_cart_bound , pose_cart_bound , pose_cart_bin , device=device)\n",
    "        pose_pole_grid = torch.linspace(-pose_pole_bound , pose_pole_bound , pose_pole_bin , device=device)\n",
    "        vel_cart_grid = torch.linspace(-vel_cart_bound , vel_cart_bound , vel_cart_bin , device=device)\n",
    "        vel_pole_grid = torch.linspace(-vel_pole_bound , vel_pole_bound , vel_pole_bin , device=device)\n",
    "\n",
    "        # digitalize to range\n",
    "        pose_cart_dig = torch.bucketize(pose_cart_clip,pose_cart_grid)\n",
    "        pose_pole_dig = torch.bucketize(pose_pole_clip,pose_pole_grid)\n",
    "        vel_cart_dig = torch.bucketize(vel_cart_clip,vel_cart_grid)\n",
    "        vel_pose_dig = torch.bucketize(vel_pole_clip,vel_pole_grid)\n",
    "\n",
    "        return ( int(pose_cart_dig), int(pose_pole_dig), int(vel_cart_dig),  int(vel_pose_dig))\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Define the number of bins for each variable in discrete space:\n",
    "    - The observation consists of four continuous variables:\n",
    "        - pose_cart (cart position)\n",
    "        - pose_pole (pole angle)\n",
    "        - vel_cart (cart velocity)\n",
    "        - vel_pole (pole angular velocity)\n",
    "    - The number of bins for each variable is determined by `self.discretize_state_weight`, which contains the bin count for each variable.\n",
    "\n",
    "2. Define maximum boundary values for clipping:\n",
    "    - To prevent extreme values from distorting the discretization process, we clip each variable to a fixed range:\n",
    "\n",
    "3. Clip the continuous values to the defined boundaries:\n",
    "    - The function extracts continuous values from `obs['policy']` and ensures they stay within the predefined range using `torch.clip()`. This prevents values from exceeding the expected range.\n",
    "\n",
    "4. Create uniform grids for discretization:\n",
    "    - Using `torch.linspace()`, the function generates evenly spaced grid points that divide the clipped continuous space into discrete bins. Each variable's range is divided into a specified number of bins.\n",
    "\n",
    "5. Discretize (bucketize) the continuous values into uniform ranges:\n",
    "    - `torch.bucketize()` assigns the clipped continuous values to their respective bins.\n",
    "    This ensures each variable is mapped to an integer index starting from 0, making it suitable for discrete RL algorithms.\n",
    "    Return the discrete state tuple:\n",
    "\n",
    "6. The function returns a tuple, each representing a discretized version of the original continuous state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **get_discretize_action(self, obs_dis)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Get the action index or action in the discrete domain from the policy by using the `Epsilon-Greedy`. This method selects an action either by exploring (choosing a random action with probability ε) or by exploiting (choosing the action with the highest reward with probability 1 - ε)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "    def get_discretize_action(self, obs_dis) -> int:\n",
    "        rand = np.random.rand() # random with unitform distribution\n",
    "\n",
    "        if(rand <= self.epsilon): # Exploration [Random action]\n",
    "            return np.random.randint(0, self.num_of_action)\n",
    "        elif (rand > self.epsilon): # Exploitation [Greedy action]\n",
    "            return np.argmax(self.q_values[obs_dis]) \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Get discrete state input to select the action\n",
    "- random a value to compare with epsilon to selects policy\n",
    "    - if **Exploration** or equal-less than epsilon : \n",
    "        - random action with random action policy\n",
    "    - else if **Exploitation** : \n",
    "        - using learning policy to select action by find a current optimal value by searching maximum action value "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **mapping_action(self, action)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Maps a discrete action in range [0, n] to a continuous value in [action_min, action_max]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "    def mapping_action(self, action):\n",
    "        \"\"\"\n",
    "        Maps a discrete action in range [0, n] to a continuous value in [action_min, action_max].\n",
    "\n",
    "        Args:\n",
    "            action (int): Discrete action in range [0, n]\n",
    "            n (int): Number of discrete actions\n",
    "        \n",
    "        Returns:\n",
    "            torch.Tensor: Scaled action tensor.\n",
    "        \"\"\"\n",
    "        # ========= put your code here =========#\n",
    "        return torch.tensor([[action * ((self.action_range[1] - self.action_range[0]) / (self.num_of_action-1 )) + self.action_range[0]]])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **decay_epsilon(self)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Decreases epsilon over time and returns the updated value.\n",
    "\n",
    "```python\n",
    "    def decay_epsilon(self):\n",
    "        \"\"\"\n",
    "        Decay epsilon value to reduce exploration over time.\n",
    "        \"\"\"\n",
    "        self.epsilon = max(self.final_epsilon ,self.epsilon-self.epsilon_decay)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2. Learning Algorithm**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Monte Carlo class**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Monte Carlo is updating always update value after episode is ending\n",
    "    - episode will end when **`env.step(action)`** return `terminate` or `truncate`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "    def update(self,obs_dis,action_idx,reward,done):\n",
    "        self.obs_hist.append(obs_dis)\n",
    "        self.action_hist.append(action_idx)\n",
    "        self.reward_hist.append(reward)\n",
    "        \n",
    "        if done:\n",
    "            G_cum = 0 # return\n",
    "            for t in range(len(self.obs_hist) - 1, -1, -1): # Loop from the last state to the first state [T-1, T-2, ..., 0]\n",
    "                G_cum = self.discount_factor * G_cum + self.reward_hist[t]\n",
    "                if (self.obs_hist[t], self.action_hist[t]) not in list(zip(self.obs_hist[:t], self.action_hist[:t])):   # if First Visit\n",
    "                    self.n_values[self.obs_hist[t]][self.action_hist[t]] += 1\n",
    "                    self.q_values[self.obs_hist[t]][self.action_hist[t]] += (G_cum - self.q_values[self.obs_hist[t]][self.action_hist[t]]) / self.n_values[self.obs_hist[t]][self.action_hist[t]]\n",
    "                    \n",
    "            self.obs_hist.clear()\n",
    "            self.action_hist.clear()\n",
    "            self.reward_hist.clear()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- At the end of an episode, we need to reset the cumulative return to 0 and loop in reverse to compute the actual return from the agent's experience.\n",
    "- When the agent visits a state for the first time in an episode, it must increment the state count and update the action value using the default action value update equation. However, there are special conditions for updating, such as not updating at every step.\n",
    "\n",
    "$$\n",
    "Q(S_t,A_t) = Q(S_t,A_t) +  \\frac{G_t - q(S_t,A_t)}{N(S_t,A_t)}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **SARSA class**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- SARSA is updating policy every timestep to improve policy by using boostraping next state from experience"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "    def update(self,obs_dis,action_idx,reward,next_obs_dis,done):\n",
    "        if done: # not boostraping\n",
    "            self.q_values[obs_dis][action_idx] += self.lr * (reward - self.q_values[obs_dis][action_idx])\n",
    "        else:\n",
    "            _next_action_idx = self.get_discretize_action(next_obs_dis)\n",
    "            self.q_values[obs_dis][action_idx] += self.lr * (reward + self.discount_factor * self.q_values[next_obs_dis][_next_action_idx] - self.q_values[obs_dis][action_idx])\n",
    "        pass\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- if episode is not terminate will boostrapping and update q value with temperal differnce term"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**First Run**\n",
    "Q Learning\n",
    "```python\n",
    "    # hyperparameters\n",
    "    num_of_action = 10\n",
    "    action_range = [-5, 5]  # [min, max]\n",
    "    discretize_state_weight = [10, 20, 10, 10]  # [pose_cart:int, pose_pole:int, vel_cart:int, vel_pole:int]\n",
    "    learning_rate = 0.3\n",
    "    n_episodes = 5000\n",
    "    start_epsilon = 1.0\n",
    "    epsilon_decay = 0.001  # reduce the exploration over time\n",
    "    final_epsilon = 0.05\n",
    "    discount = 1\n",
    "```\n",
    "\n",
    "- agent sometime tried to force cart to make pole to center but  often agent is do nothing\n",
    "- when see in q value json file I can see most of state is not explore by agent\n",
    "\n",
    "\n",
    "**Run 2**\n",
    "```python\n",
    "    # hyperparameters\n",
    "    num_of_action = 4\n",
    "    action_range = [-3, 3]  # [min, max]\n",
    "    discretize_state_weight = [8, 12, 4, 4]  # [pose_cart:int, pose_pole:int, vel_cart:int, vel_pole:int]\n",
    "    learning_rate = 0.3\n",
    "    n_episodes = 3000\n",
    "    start_epsilon = 1.0\n",
    "    epsilon_decay = 0.001  # reduce the exploration over time\n",
    "    final_epsilon = 0.05\n",
    "    discount = 1\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "QL RUN 3 OVER SPACE"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
